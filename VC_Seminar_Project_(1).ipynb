{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7d8e9827-67c7-4df3-a33f-6b39b242b708",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "db15d686ea9ac6920835a9486010698f",
          "grade": false,
          "grade_id": "cell-50142891b18d9a68",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "7d8e9827-67c7-4df3-a33f-6b39b242b708"
      },
      "source": [
        "<img src=\"seminarlogo.png\" class=\"center\" style=\"width:100%;\"> </img>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c50e015-a14b-4bcf-8796-45f88d5b1c93",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "70a38740a3ea3ef9e9f02091d93df34f",
          "grade": false,
          "grade_id": "cell-f8f56a83dc8fea6c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "0c50e015-a14b-4bcf-8796-45f88d5b1c93"
      },
      "source": [
        "# <span style=\"color:navy\"> **Seminar Project:** </span> <span style=\"color:green\"> **Video Coding**</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee3ef06-b97e-4b31-9bdc-d5697b42b9a2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jp-MarkdownHeadingCollapsed": true,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e2a2e8935fbd0a88c3ef01a1f3595805",
          "grade": false,
          "grade_id": "cell-b6c65d68479e9e79",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "7ee3ef06-b97e-4b31-9bdc-d5697b42b9a2"
      },
      "source": [
        "<div style=\"background-color:#ebeff7; padding:1.25em; border-radius:1em; border: 1px solid black\">\n",
        "\n",
        "### <span style = \"color: navy\"> **General Procedure:** </span>\n",
        "1. Make group of $2$ or $3$ students while coordinating with each other and select one group coordinator.\n",
        "2. Select one the techniques (MDCT or Lapped Transform)\n",
        "4. Send us the the selected task (technique) and the list of group members with their <b>a)</b> Matriculation Numbers <b>b)</b> email IDs, and highlighting the group coordinator. Make it in a **table** form.\n",
        "5. The duration of the seminar project is from **11.06.2024** to **31.07.2024**, with a mid-term meeting with each group from *01.07.2024 to 06.07.2024* (Book an appointment at your convenience)\n",
        "6. There will be a report at the end of project. The submission date for the report is **31.07.2024**\n",
        "7. The report and results presentation will held from *01.08.2024 to 15.08.2024* (The specific date for each group will be selected after mutual consensus)\n",
        "\n",
        "\n",
        "<span style=\"color:blue\"> Note: Further Instructions and procedures may be updated from time to time</span>\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a78ab026-535c-405a-873f-8eea172670db",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7cea22af77a68148302a9971e62f249b",
          "grade": false,
          "grade_id": "cell-3728701544a83df4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "a78ab026-535c-405a-873f-8eea172670db"
      },
      "source": [
        "## <span style = \"color: navy\"> **Introduction:** </span>\n",
        "There are two seminar projects, with the aim to compress the given images (Only use the given image in the base repository). The projects are:\n",
        "<br>\n",
        "### **Project-1:** <span style = \"color: green\"> Image-Compression using MDCT </span>\n",
        "### **Project-2:** <span style = \"color: green\"> Image-Compression using Lapped Transform </span>\n",
        "\n",
        "The projects are team-based work with a group of **2** (minimum) to **3** (maximum) students. You (as a team) are required to select one project and implement the technique (either *MDCT* or *Lapped* Transform). <span style = \"color: red\">To make a balance in project selection, we can assign either Project-1 or Project-2 to any group.</span>\n",
        "\n",
        "### <span style = \"color: navy\"> GET STARTED </span>\n",
        "The basic building blocks (base algorithm and/or material) of the Projects are provided in the git-repository (https://github.com/Karanraj06/image-compression).\n",
        "In this repository, the implementation of image compression is done by using simple **DCT** which you can take as your staring point and further develop or modify this base-repository according to the selected project i.e. techniques (*MDCT* or *Lapped* Transform) of selected project.\n",
        "\n",
        "### <span style = \"color: navy\"> Project Tasks </span>\n",
        "1. Take the base repository as base or reference\n",
        "2. Modify the DCT part with your selected technique (either MDCT or Lapped Transform)\n",
        "    - For the \"MDCT\" you need to find out the optimum window, try different windows\n",
        "    - The \"lapped transform\" needs to be implemented by Pytorch's Conv2D. For that, the kernel size can be chosen like the MDCT, which is 16x16. In order to get good transform filters, it needs to be trained, and for that you need to use the images in the image coder repository.\n",
        "5. Compare the results of compression of your modified version with the reference, which is DCT, by using Perceptual Similarity Metric and compression ratio. The repository for Perceptual Similarity Metric measurement is given here (https://github.com/richzhang/PerceptualSimilarity)\n",
        "6. The expected calculations among others should include the results of \"bits per pixel\"\n",
        "\n",
        "### <span style = \"color: navy\"> Some use full links </span>\n",
        "1. https://github.com/TUIlmenauAMS/Python-Audio-Coder\n",
        "2. https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "\n",
        "##### <span style=\"color:blue\"> Further details will be discussed with the group coordinators and the they are responsible for group coordination</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02c732dd-751d-4e75-9801-dc9cbc89a373",
      "metadata": {
        "tags": [],
        "id": "02c732dd-751d-4e75-9801-dc9cbc89a373"
      },
      "source": [
        "<div style=\"background-color:#ebeff7; padding:1.25em; border-radius:1em; border: 1px solid black\">\n",
        "\n",
        "### <span style=\"color:navy; font-size: 1.25em\"> **Project:** </span>\n",
        "#### <span style=\"color:blue\"> Below Enter Matriculation Numbers of each member </span>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43c3d7c7-4682-4ba3-b107-5242a0c8a8a5",
      "metadata": {
        "tags": [],
        "id": "43c3d7c7-4682-4ba3-b107-5242a0c8a8a5"
      },
      "outputs": [],
      "source": [
        "MatriculationNumber = '64684, 66512, 123456'; # Example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d525ded0-8652-431b-b5a9-8d70a2deaa78",
      "metadata": {
        "tags": [],
        "id": "d525ded0-8652-431b-b5a9-8d70a2deaa78"
      },
      "source": [
        "### <span style=\"color:navy; font-size: 1.25em\"> **Description:**</span>\n",
        "Place to describe the task, procedure, results etc. (delete this line when writting)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage import color\n",
        "from skimage import io\n",
        "from scipy.fftpack import dct, idct\n",
        "from torchvision import transforms\n",
        "from scipy.signal import convolve2d\n",
        "import os"
      ],
      "metadata": {
        "id": "d-May_6-GMYQ"
      },
      "id": "d-May_6-GMYQ",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im = cv2.imread('./ein.png', cv2.IMREAD_GRAYSCALE)\n",
        "f_image = np.float32(im)\n",
        "# Apply DCT\n",
        "dct_image = cv2.dct(f_image)\n",
        "\n",
        "# Amplify high-frequency components (optional, adjust as needed)\n",
        "dct_image *= 2  # Experiment with different values for quality adjustment\n",
        "\n",
        "# Apply inverse DCT\n",
        "idct_image = cv2.idct(dct_image)\n",
        "\n",
        "# Clip values to valid range and convert back to uint8\n",
        "idct_image_clipped = np.clip(idct_image, 0, 255).astype(np.uint8)\n",
        "# cv2.imread('')\n",
        "# Save the compressed image\n",
        "cv2.imwrite('./compressed_image.png', idct_image_clipped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KbkIcagGFAX",
        "outputId": "b97eeb9e-9704-40f5-919d-c60dd50e9216"
      },
      "id": "3KbkIcagGFAX",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/drive/MyDrive/image-compression"
      ],
      "metadata": {
        "id": "hrQGjfBbjGAp"
      },
      "id": "hrQGjfBbjGAp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "43da85af-347b-40b8-bf31-5a05bca167c4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4dc50680a95fdd1d5a311ed82e8fbb2a",
          "grade": false,
          "grade_id": "cell-c6e9ec1a38c4bda2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "43da85af-347b-40b8-bf31-5a05bca167c4"
      },
      "source": [
        "### <span style=\"color:blue\"> END Report here </span>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_save_dct_images(image_paths):\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import os\n",
        "    paths = []\n",
        "    files = os.listdir(image_paths)\n",
        "    for i in files:\n",
        "      image_path = os.path.join(image_paths, i)\n",
        "      paths.append(image_path)\n",
        "    for image_ in paths:\n",
        "      # print(image_)\n",
        "\n",
        "    # output_path = os.path.join(image_paths, 'compressed'\n",
        "    # for image_path in image_paths:\n",
        "        # print(image_path)\n",
        "        # Load image as grayscale\n",
        "      im = cv2.imread(image_, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # # cv2.imshow('DCT Image', im)  # Use log scale for better visualization\n",
        "        # # cv2.imshow('Reconstructed Image', np.uint8(reconstructed_image))\n",
        "        # # cv2.waitKey(0)\n",
        "        # # cv2.destroyAllWindows() # Check if the image was successfully loaded\n",
        "      if im is None:\n",
        "          print(f\"Error: Unable to load image at {image_paths}. Check the file path.\")\n",
        "          return\n",
        "\n",
        "      # Print image type and shape for debugging\n",
        "      # print(f\"Loaded image type: {type(im)}, shape: {im.shape}, dtype: {im.dtype}\")\n",
        "      f_image = np.float32(im)\n",
        "        # # Apply DCT\n",
        "      dct_image = cv2.dct(f_image)\n",
        "\n",
        "        # # Amplify high-frequency components (optional, adjust as needed)\n",
        "      dct_image *= 2  # Experiment with different values for quality adjustment\n",
        "\n",
        "        # # Apply inverse DCT\n",
        "      idct_image = cv2.idct(dct_image)\n",
        "\n",
        "        # # Clip values to valid range and convert back to uint8\n",
        "      idct_image_clipped = np.clip(idct_image, 0, 255).astype(np.uint8)\n",
        "      os.makedirs('/content/drive/MyDrive/image-compression/dct_color', exist_ok=True)\n",
        "      print(f'Saving image: img_{os.path.split(image_)[-1]}')\n",
        "      cv2.imwrite(f'/content/drive/MyDrive/image-compression/dct_color/{os.path.split(image_)[-1]}', idct_image_clipped)\n",
        "      # j = j + 1\n",
        "\n",
        "\n",
        "    print('ALL DOne')\n",
        "        # # Save the compressed image\n",
        "        # filename = os.path.basename(image_path)\n",
        "        # to_save = os.path.join(output_path, filename)\n",
        "        # print(f\"Saving: {to_save}\")\n",
        "        # cv2.imwrite(to_save, idct_image_clipped)\n",
        "\n",
        "process_and_save_dct_images(image_paths = './image-compression/images/colored')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJmzHtZ_E0au",
        "outputId": "a5f045c6-d5a6-4e86-e551-44b134da93cd"
      },
      "id": "nJmzHtZ_E0au",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving image: img_9.png\n",
            "Saving image: img_6.png\n",
            "Saving image: img_20.png\n",
            "Saving image: img_22.png\n",
            "Saving image: img_10.png\n",
            "Saving image: img_14.png\n",
            "Saving image: img_21.png\n",
            "Saving image: img_23.png\n",
            "Saving image: img_24.png\n",
            "Saving image: img_3.png\n",
            "Saving image: img_8.png\n",
            "Saving image: img_5.png\n",
            "Saving image: img_11.png\n",
            "Saving image: img_7.png\n",
            "Saving image: img_12.png\n",
            "Saving image: img_2.png\n",
            "Saving image: img_1.png\n",
            "Saving image: img_19.png\n",
            "Saving image: img_13.png\n",
            "Saving image: img_16.png\n",
            "Saving image: img_18.png\n",
            "Saving image: img_4.png\n",
            "Saving image: img_15.png\n",
            "Saving image: img_17.png\n",
            "ALL DOne\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_save_dct_images(image_paths):\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import os\n",
        "    paths = []\n",
        "    files = os.listdir(image_paths)\n",
        "    for i in files:\n",
        "      image_path = os.path.join(image_paths, i)\n",
        "      paths.append(image_path)\n",
        "    for image_ in paths:\n",
        "      # print(image_)\n",
        "\n",
        "    # output_path = os.path.join(image_paths, 'compressed'\n",
        "    # for image_path in image_paths:\n",
        "        # print(image_path)\n",
        "        # Load image as grayscale\n",
        "      im = cv2.imread(image_, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # # cv2.imshow('DCT Image', im)  # Use log scale for better visualization\n",
        "        # # cv2.imshow('Reconstructed Image', np.uint8(reconstructed_image))\n",
        "        # # cv2.waitKey(0)\n",
        "        # # cv2.destroyAllWindows() # Check if the image was successfully loaded\n",
        "      if im is None:\n",
        "          print(f\"Error: Unable to load image at {image_paths}. Check the file path.\")\n",
        "          return\n",
        "\n",
        "      # Print image type and shape for debugging\n",
        "      # print(f\"Loaded image type: {type(im)}, shape: {im.shape}, dtype: {im.dtype}\")\n",
        "      f_image = np.float32(im)\n",
        "        # # Apply DCT\n",
        "      dct_image = cv2.dct(f_image)\n",
        "\n",
        "        # # Amplify high-frequency components (optional, adjust as needed)\n",
        "      dct_image *= 2  # Experiment with different values for quality adjustment\n",
        "\n",
        "        # # Apply inverse DCT\n",
        "      idct_image = cv2.idct(dct_image)\n",
        "\n",
        "        # # Clip values to valid range and convert back to uint8\n",
        "      idct_image_clipped = np.clip(idct_image, 0, 255).astype(np.uint8)\n",
        "      os.makedirs('/content/drive/MyDrive/image-compression/dct_grayscale', exist_ok=True)\n",
        "      print(f'Saving image: img_{os.path.split(image_)[-1]}')\n",
        "      cv2.imwrite(f'/content/drive/MyDrive/image-compression/dct_grayscale/{os.path.split(image_)[-1]}', idct_image_clipped)\n",
        "      # j = j + 1\n",
        "\n",
        "\n",
        "    print('ALL DOne')\n",
        "        # # Save the compressed image\n",
        "        # filename = os.path.basename(image_path)\n",
        "        # to_save = os.path.join(output_path, filename)\n",
        "        # print(f\"Saving: {to_save}\")\n",
        "        # cv2.imwrite(to_save, idct_image_clipped)\n",
        "\n",
        "process_and_save_dct_images(image_paths = './image-compression/images/grayscale')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m1xy95gjsyH",
        "outputId": "9462e51b-f0ab-4aa5-aa7a-0b0d39b2da35"
      },
      "id": "0m1xy95gjsyH",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving image: img_4.tif\n",
            "Saving image: img_6.tif\n",
            "Saving image: img_3.tif\n",
            "Saving image: img_10.tif\n",
            "Saving image: img_2.tif\n",
            "Saving image: img_7.tif\n",
            "Saving image: img_5.tif\n",
            "Saving image: img_1.tif\n",
            "Saving image: img_8.tif\n",
            "Saving image: img_9.tif\n",
            "ALL DOne\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "90597b40-56ee-45c3-a2b1-af505cd0bc7f",
      "metadata": {
        "id": "90597b40-56ee-45c3-a2b1-af505cd0bc7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b53021c-6ae3-4341-92b8-86a726f264d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving image: 9.png\n",
            "Saving image: 6.png\n",
            "Saving image: 20.png\n",
            "Saving image: 22.png\n",
            "Saving image: 10.png\n",
            "Saving image: 14.png\n",
            "Saving image: 21.png\n",
            "Saving image: 23.png\n",
            "Saving image: 24.png\n",
            "Saving image: 3.png\n",
            "Saving image: 8.png\n",
            "Saving image: 5.png\n",
            "Saving image: 11.png\n",
            "Saving image: 7.png\n",
            "Saving image: 12.png\n",
            "Saving image: 2.png\n",
            "Saving image: 1.png\n",
            "Saving image: 19.png\n",
            "Saving image: 13.png\n",
            "Saving image: 16.png\n",
            "Saving image: 18.png\n",
            "Saving image: 4.png\n",
            "Saving image: 15.png\n",
            "Saving image: 17.png\n"
          ]
        }
      ],
      "source": [
        "# Function to perform DCT and IDCT with overlap\n",
        "def lapped_transform(im_np, block_size=8, overlap=4):\n",
        "    STEP_SIZE = block_size - overlap\n",
        "    height, width = im_np.shape\n",
        "    step_size = block_size - overlap\n",
        "\n",
        "    # Padding the image\n",
        "    pad_height = (step_size - height % step_size) % step_size\n",
        "    pad_width = (step_size - width % step_size) % step_size\n",
        "    im_np_padded = np.pad(im_np, ((0, pad_height), (0, pad_width)), mode='constant')\n",
        "\n",
        "    # Initialize arrays for compressed and decompressed data\n",
        "    compressed = np.zeros_like(im_np_padded, dtype=np.float32)\n",
        "    count = np.zeros_like(im_np_padded, dtype=np.float32)\n",
        "\n",
        "    # Process each block with overlap\n",
        "    for i in range(0, im_np_padded.shape[0] - overlap, step_size):\n",
        "        for j in range(0, im_np_padded.shape[1] - overlap, step_size):\n",
        "            block = im_np_padded[i:i + block_size, j:j + block_size]\n",
        "\n",
        "            # Apply DCT\n",
        "            dct_block = dct(dct(block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
        "\n",
        "            # (Optional) Quantization and compression can be applied here\n",
        "\n",
        "            # Apply inverse DCT\n",
        "            idct_block = idct(idct(dct_block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
        "\n",
        "            # Overlap-add and average\n",
        "            compressed[i:i + block_size, j:j + block_size] += idct_block\n",
        "            count[i:i + block_size, j:j + block_size] += 1\n",
        "\n",
        "    # Avoid division by zero\n",
        "    count[count == 0] = 1\n",
        "    compressed /= count\n",
        "\n",
        "    # Clip values to valid range\n",
        "    compressed_clipped = np.clip(compressed, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return compressed_clipped\n",
        "\n",
        "# Function to process and save Lapped Transform compressed images\n",
        "def process_and_save_lapped_transform_images(image_paths):\n",
        "    paths = []\n",
        "    files = os.listdir(image_paths)\n",
        "    for i in files:\n",
        "      image_path = os.path.join(image_paths, i)\n",
        "      paths.append(image_path)\n",
        "    for image_ in paths:\n",
        "      im = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "      f_image = np.float32(im)\n",
        "\n",
        "      # Apply Lapped Transform\n",
        "      compressed_image = lapped_transform(f_image)\n",
        "      os.makedirs('/content/drive/MyDrive/image-compression/lapped_color/', exist_ok=True)\n",
        "      print(f'Saving image: {os.path.split(image_)[-1]}')\n",
        "      cv2.imwrite(f'/content/drive/MyDrive/image-compression/lapped_color/{os.path.split(image_)[-1]}', idct_image_clipped)\n",
        "\n",
        "process_and_save_lapped_transform_images(image_paths = './image-compression/images/colored')\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform DCT and IDCT with overlap\n",
        "def lapped_transform(im_np, block_size=8, overlap=4):\n",
        "    STEP_SIZE = block_size - overlap\n",
        "    height, width = im_np.shape\n",
        "    step_size = block_size - overlap\n",
        "\n",
        "    # Padding the image\n",
        "    pad_height = (step_size - height % step_size) % step_size\n",
        "    pad_width = (step_size - width % step_size) % step_size\n",
        "    im_np_padded = np.pad(im_np, ((0, pad_height), (0, pad_width)), mode='constant')\n",
        "\n",
        "    # Initialize arrays for compressed and decompressed data\n",
        "    compressed = np.zeros_like(im_np_padded, dtype=np.float32)\n",
        "    count = np.zeros_like(im_np_padded, dtype=np.float32)\n",
        "\n",
        "    # Process each block with overlap\n",
        "    for i in range(0, im_np_padded.shape[0] - overlap, step_size):\n",
        "        for j in range(0, im_np_padded.shape[1] - overlap, step_size):\n",
        "            block = im_np_padded[i:i + block_size, j:j + block_size]\n",
        "\n",
        "            # Apply DCT\n",
        "            dct_block = dct(dct(block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
        "\n",
        "            # (Optional) Quantization and compression can be applied here\n",
        "\n",
        "            # Apply inverse DCT\n",
        "            idct_block = idct(idct(dct_block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
        "\n",
        "            # Overlap-add and average\n",
        "            compressed[i:i + block_size, j:j + block_size] += idct_block\n",
        "            count[i:i + block_size, j:j + block_size] += 1\n",
        "\n",
        "    # Avoid division by zero\n",
        "    count[count == 0] = 1\n",
        "    compressed /= count\n",
        "\n",
        "    # Clip values to valid range\n",
        "    compressed_clipped = np.clip(compressed, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return compressed_clipped\n",
        "\n",
        "# Function to process and save Lapped Transform compressed images\n",
        "def process_and_save_lapped_transform_images(image_paths):\n",
        "    paths = []\n",
        "    files = os.listdir(image_paths)\n",
        "    for i in files:\n",
        "      image_path = os.path.join(image_paths, i)\n",
        "      paths.append(image_path)\n",
        "    for image_ in paths:\n",
        "      im = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "      f_image = np.float32(im)\n",
        "\n",
        "      # Apply Lapped Transform\n",
        "      compressed_image = lapped_transform(f_image)\n",
        "      os.makedirs('/content/drive/MyDrive/image-compression/lapped_scale/', exist_ok=True)\n",
        "      print(f'Saving image: {os.path.split(image_)[-1]}')\n",
        "      cv2.imwrite(f'/content/drive/MyDrive/image-compression/lapped_scale/{os.path.split(image_)[-1]}', idct_image_clipped)\n",
        "\n",
        "process_and_save_lapped_transform_images(image_paths = './image-compression/images/grayscale')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWn4F--nkvu6",
        "outputId": "69f2a36f-4ffa-4148-8873-9c064d1c1884"
      },
      "id": "UWn4F--nkvu6",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving image: 4.tif\n",
            "Saving image: 6.tif\n",
            "Saving image: 3.tif\n",
            "Saving image: 10.tif\n",
            "Saving image: 2.tif\n",
            "Saving image: 7.tif\n",
            "Saving image: 5.tif\n",
            "Saving image: 1.tif\n",
            "Saving image: 8.tif\n",
            "Saving image: 9.tif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate compression ratio for each image\n",
        "def get_file_size(file_path):\n",
        "    return os.path.getsize(file_path) if os.path.isfile(file_path) else 0\n",
        "def calculate_compression_ratios(original_path):\n",
        "    ratios = []\n",
        "    for filename in os.listdir(original_path):\n",
        "        original_image_path = os.path.join(original_path, filename)\n",
        "        dct_image_path = os.path.join('/content/drive/MyDrive/image-compression/dct_color', filename)\n",
        "        lbt_image_path = os.path.join('/content/drive/MyDrive/image-compression/lapped_color/', filename)\n",
        "\n",
        "\n",
        "\n",
        "        # File sizes\n",
        "        original_size = get_file_size(original_image_path)\n",
        "        dct_size = get_file_size(dct_image_path)\n",
        "        lbt_size = get_file_size(lbt_image_path)\n",
        "\n",
        " # Compression ratios\n",
        "        ratio_dct = original_size / dct_size if dct_size > 0 else 0\n",
        "        ratio_lbt = original_size / lbt_size if lbt_size > 0 else 0\n",
        "\n",
        "        ratios.append({\n",
        "            'Filename': filename,\n",
        "            'Original Size (bytes)': original_size,\n",
        "            'DCT Size (bytes)': dct_size,\n",
        "            'Lapped Size (bytes)': lbt_size,\n",
        "            'DCT Compression Ratio': ratio_dct,\n",
        "            'Lapped Compression Ratio': ratio_lbt\n",
        "        })\n",
        "    import pandas as pd\n",
        "    # pd.set_option('display.max_colwidth', None)\n",
        "    df = pd.DataFrame(ratios)\n",
        "    df.to_csv('/content/drive/MyDrive/image-compression/compression_ratios_color.csv', index=False)\n",
        "    # print(df)\n",
        "\n",
        "    return\n",
        "\n",
        "calculate_compression_ratios(original_path = './image-compression/images/colored')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EkwjpYVYSUUO"
      },
      "id": "EkwjpYVYSUUO",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate compression ratio for each image\n",
        "def get_file_size(file_path):\n",
        "    return os.path.getsize(file_path) if os.path.isfile(file_path) else 0\n",
        "def calculate_compression_ratios(original_path):\n",
        "    ratios = []\n",
        "    for filename in os.listdir(original_path):\n",
        "        original_image_path = os.path.join(original_path, filename)\n",
        "        dct_image_path = os.path.join('/content/drive/MyDrive/image-compression/dct_grayscale/', filename)\n",
        "        lbt_image_path = os.path.join('/content/drive/MyDrive/image-compression/lapped_scale/', filename)\n",
        "\n",
        "\n",
        "\n",
        "        # File sizes\n",
        "        original_size = get_file_size(original_image_path)\n",
        "        dct_size = get_file_size(dct_image_path)\n",
        "        lbt_size = get_file_size(lbt_image_path)\n",
        "\n",
        " # Compression ratios\n",
        "        ratio_dct = original_size / dct_size if dct_size > 0 else 0\n",
        "        ratio_lbt = original_size / lbt_size if lbt_size > 0 else 0\n",
        "\n",
        "        ratios.append({\n",
        "            'Filename': filename,\n",
        "            'Original Size (bytes)': original_size,\n",
        "            'DCT Size (bytes)': dct_size,\n",
        "            'Lapped Size (bytes)': lbt_size,\n",
        "            'DCT Compression Ratio': ratio_dct,\n",
        "            'Lapped Compression Ratio': ratio_lbt\n",
        "        })\n",
        "    import pandas as pd\n",
        "    # pd.set_option('display.max_colwidth', None)\n",
        "    df = pd.DataFrame(ratios)\n",
        "    df.to_csv('/content/drive/MyDrive/image-compression/compression_ratios_grayscale.csv', index=False)\n",
        "    # print(df)\n",
        "\n",
        "    return\n",
        "\n",
        "calculate_compression_ratios(original_path = './image-compression/images/grayscale')"
      ],
      "metadata": {
        "id": "vCqDfvcql8Rh"
      },
      "id": "vCqDfvcql8Rh",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lpips\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAMWNF_PXxWl",
        "outputId": "61f33800-85bc-45e4-813b-5381bafe37a3"
      },
      "id": "DAMWNF_PXxWl",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.18.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.0->lpips) (12.6.20)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import lpips\n",
        "import torch\n",
        "import pandas as pd\n",
        "# import path"
      ],
      "metadata": {
        "id": "u-KHZ-LLXosF"
      },
      "id": "u-KHZ-LLXosF",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calculate_lpips(original_path, compressed_path):\n",
        "    total_lpips = 0.0\n",
        "    image_count = 0\n",
        "    data = []\n",
        "    transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize for uniformity\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    for filename in os.listdir(original_path):\n",
        "        original_image_path = os.path.join(original_path, filename)\n",
        "        compressed_image_path = os.path.join(compressed_path, filename)\n",
        "\n",
        "        # Check if corresponding compressed image exists and skip directories\n",
        "        if not os.path.isfile(original_image_path) or not os.path.isfile(compressed_image_path):\n",
        "            continue\n",
        "\n",
        "        # Load images\n",
        "        original_image = Image.open(original_image_path).convert('RGB')\n",
        "        compressed_image = Image.open(compressed_image_path).convert('RGB')\n",
        "\n",
        "        # Transform images\n",
        "        original_tensor = transform(original_image).unsqueeze(0)\n",
        "        compressed_tensor = transform(compressed_image).unsqueeze(0)\n",
        "\n",
        "        # Calculate LPIPS\n",
        "        lpips_model = lpips.LPIPS(net='alex')\n",
        "        lpips_value = lpips_model(original_tensor, compressed_tensor)\n",
        "        lpips_value_item = lpips_value.item()\n",
        "        total_lpips += lpips_value_item\n",
        "        image_count += 1\n",
        "\n",
        "        #print(f\"LPIPS for {filename}: {lpips_value.item()}\")\n",
        "\n",
        "        # Store result\n",
        "        data.append({'File Name':filename, 'LPIPS Value':lpips_value_item})\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('/content/drive/MyDrive/image-compression/lpips_color_results.csv', index=False)\n",
        "    return\n",
        "\n",
        "calculate_lpips(original_path = '/content/drive/MyDrive/image-compression/dct_color', compressed_path = '/content/drive/MyDrive/image-compression/lapped_color')\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVP6OXQ8WKFF",
        "outputId": "aeb14b86-a947-414a-bec1-449c50d04675"
      },
      "id": "gVP6OXQ8WKFF",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calculate_lpips(original_path, compressed_path):\n",
        "    total_lpips = 0.0\n",
        "    image_count = 0\n",
        "    data = []\n",
        "    transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize for uniformity\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    for filename in os.listdir(original_path):\n",
        "        original_image_path = os.path.join(original_path, filename)\n",
        "        compressed_image_path = os.path.join(compressed_path, filename)\n",
        "\n",
        "        # Check if corresponding compressed image exists and skip directories\n",
        "        if not os.path.isfile(original_image_path) or not os.path.isfile(compressed_image_path):\n",
        "            continue\n",
        "\n",
        "        # Load images\n",
        "        original_image = Image.open(original_image_path).convert('RGB')\n",
        "        compressed_image = Image.open(compressed_image_path).convert('RGB')\n",
        "\n",
        "        # Transform images\n",
        "        original_tensor = transform(original_image).unsqueeze(0)\n",
        "        compressed_tensor = transform(compressed_image).unsqueeze(0)\n",
        "\n",
        "        # Calculate LPIPS\n",
        "        lpips_model = lpips.LPIPS(net='alex')\n",
        "        lpips_value = lpips_model(original_tensor, compressed_tensor)\n",
        "        lpips_value_item = lpips_value.item()\n",
        "        total_lpips += lpips_value_item\n",
        "        image_count += 1\n",
        "\n",
        "        #print(f\"LPIPS for {filename}: {lpips_value.item()}\")\n",
        "\n",
        "        # Store result\n",
        "        data.append({'File Name':filename, 'LPIPS Value':lpips_value_item})\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('/content/drive/MyDrive/image-compression/lpips_gray_results.csv', index=False)\n",
        "    return\n",
        "\n",
        "calculate_lpips(original_path = '/content/drive/MyDrive/image-compression/dct_grayscale', compressed_path = '/content/drive/MyDrive/image-compression/lapped_scale')\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X8rKRTeZNwn",
        "outputId": "2b46e640-1961-43ea-cd07-ae2800ec1fc5"
      },
      "id": "-X8rKRTeZNwn",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bI5ROE3HnUZi"
      },
      "id": "bI5ROE3HnUZi",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}